<!DOCTYPE html>
<html lang="en">

<!-- Color: https://htmlcolorcodes.com/zh/yanse-ming/ -->

<head>
	<link rel="shortcut icon" href="files/ucla.jpg" />
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());
		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
		integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">

	<!-- Icon CSS-->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
		integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

	<!-- Custom styles for this template -->
	<link rel="stylesheet" href="files/panlu.css">
</head>


<title>Salman Rahman</title>


<body style="">
	<!-- Navigation Labels -->
	<!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top" style="background-color: #003262;"> -->
	<nav class="navbar navbar-expand-md navbar-dark fixed-top" style="background-color: #003262; font-size: 22px">

		<a class="navbar-brand" href="#" style="font-size: 22px">Pan Lu</a>

		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
			<span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#News">News</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Education">Education</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Experience">Experience</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Teaching">Teaching</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Service">Service</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Awards">Awards</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Contact">Contact</a>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container">
		<div class="col-md-3" id="div-image">
			<br>
			<img class="img-responsive img-rounded" src="files/pan.jpeg" alt=""
						style="max-width: 240px; border:1px solid black"><br>
			<!-- Icons -->
			<div style="font-size: 24px;">
				<br>
				<!-- icon size: large fa-2x, small, fa-lg -->
				<a href="mailto:lupantech@gmail.com">
					<font color="gray"><i class="fas fa-envelope fa-lg"></i></font>
				</a>&thinsp;
				<a target="_blank" href="https://scholar.google.com/citations?user=IyucsdQAAAAJ&hl=en">
					<font color="gray"><i class="ai ai-google-scholar ai-lg"></i></font>
				</a>&thinsp;
				<a target="_blank" href="https://www.semanticscholar.org/author/Pan-Lu/2887562">
					<font color="gray"><i class="ai ai-semantic-scholar ai-lg"></i></font>
				</a>&thinsp;
				<a target="_blank" href="https://github.com/lupantech">
					<font color="gray"><i class="fab fa-github fa-lg"></i></font>
				</a>&thinsp;
				<a target="_blank" href="https://www.linkedin.com/in/pan-lu-9308909a/">
					<font color="gray"><i class="fab fa-linkedin fa-lg"></i></font>
				</a>&thinsp;
				<!-- <a target="_blank" href="https://www.facebook.com/xxx"><font color="black"><i class="fab fa-facebook-square fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;&nbsp; -->
				<!-- <a target="_blank" href="files/CV_Pan Lu.pdf"><font color="black"><i class="ai ai-cv ai-lg"></i></font></a> -->
				<a target="_blank" href="https://twitter.com/lupantech">
					<font color="gray"><i class="fab fa-twitter fa-lg"></i></font>
				</a>
				<br><br>
				<div class="container" style="text-align: left; padding-left: 30px">
					<a href="https://twitter.com/lupantech?ref_src=twsrc%5Etfw" class="twitter-follow-button"
						data-show-count="false" data-size="large" >Follow @lupantech</a>
					<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				</div>
			</div>
		</div>
		<div class="container" id="div-bio">
			<div class="row">
				<div class="col-md-12">
					<br>
					<p> 
						I'm currently a final-year Ph.D. candidate in the Computer Science Department at UCLA, supervised by  <a href="http://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang</a> and <a href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Song-Chun Zhu</a>. I am a member of <a href="http://web.cs.ucla.edu/~kwchang/" target="_blank">UCLA Natural Language Processing Group (UCLA NLP)</a> and <a href="https://vcla.stat.ucla.edu/" target="_blank">the Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a>.  Previously, I received a Research Master's degree at Tsinghua University, advised by <a href="http://dbgroup.cs.tsinghua.edu.cn/wangjy/" target="_blank">Jianyong Wang</a>. My research has been funded by 
						<a href="https://www.sciencehub.ucla.edu/2023-amazon-fellows/" target="_blank">Amazon PhD Fellowship</a>, 
						Bloomberg PhD Fellowship, 
						<a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america" target="_blank">Qualcomm Innovation Fellowship</a>, 
						<a href="https://grad.ucla.edu/funding/financial-aid/funding-for-continuing-students/dissertation-year-fellowship/" target="_blank">UCLA Dissertation Year Fellowship</a>, 
						<a href="https://www.darpa.mil/program/explainable-artificial-intelligence" target="_blank">DARPA</a>,
						<a href="https://vcla.f.ucla.edu/MURI_Visual_CommonSense/" target="_blank">Naval Research Grant</a>, 
						and NeurIPS Scholar Award.

						<!-- <a href="https://www.darpa.mil/program/explainable-artificial-intelligence" target="_blank">DARPA</a>,
						<a href="https://vcla.stat.ucla.edu/MURI_Visual_CommonSense/" target="_blank">Naval Research Grant</a> -->
					</p>
					<p>
						My research goal is to build machines that can <strong>reason</strong> and <strong>collaborate</strong> with humans for the common good. My primary research focuses on <strong>machine reasoning</strong> and <strong>trustworthy NLP</strong>, particularly in the domains of mathematics, science, and medicine:
					</p>
					<ul>
						<li>
							<strong>Mathematical reasoning</strong> for problem solving and theorem proving in educational and academic contexts 
							<a href="https://mathvista.github.io/" target="_blank">[ICLR-24]</a>
							<a href="https://arxiv.org/abs/2304.09842" target="_blank">[NeurIPS-23]</a>
							<a href="https://arxiv.org/abs/2305.12524" target="_blank">[EMNLP-23]</a>
							<a href="https://arxiv.org/abs/2212.10535" target="_blank">[ACL-23]</a>
							<a href="https://arxiv.org/abs/2209.14610" target="_blank">[ICLR-23]</a>
							<a href="https://arxiv.org/abs/2210.17517" target="_blank">[EMNLP-22]</a>
							<a href="https://arxiv.org/abs/2212.02746" target="_blank">[EMNLP-22]</a>
							<a href="https://arxiv.org/abs/2110.13214" target="_blank">[NeurIPS-21]</a>
							<a href="https://arxiv.org/abs/2105.04165" target="_blank">[ACL-21]</a>
						</li>
						<li>
							<strong>Tool augmented large language models</strong> for planning, reasoning, and generation 
							<a href="https://arxiv.org/abs/2307.10635" target="_blank">[SciBench]</a> 
							<a href="https://arxiv.org/abs/2304.09842" target="_blank">[NeurIPS-23]</a>
							<a href="https://arxiv.org/abs/2305.12524" target="_blank">[EMNLP-23]</a>
							<a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/04/ArK.pdf" target="_blank">[ArK]</a> 
							<a href="https://arxiv.org/abs/2305.01795" target="_blank">[MPP]</a> 
							<a href="https://arxiv.org/abs/2210.17517" target="_blank">[EMNLP-22]</a>
						</li>
						<li>
							<!-- <strong>AI for science and education</strong>, including benchmarks, evaluations, algorithms, and tutoring systems -->
							<strong>AI for scientific reasoning and discovery</strong>, including benchmarks, algorithms, and systems
							<a href="https://arxiv.org/abs/2307.10635" target="_blank">[SciBench]</a> 
							<a href="https://arxiv.org/abs/2304.09842" target="_blank">[NeurIPS-23]</a>
							<a href="https://arxiv.org/abs/2305.12524" target="_blank">[EMNLP-23]</a>
							<a href="https://arxiv.org/abs/2209.09513" target="_blank">[NeurIPS-22]</a> 
							<a href="https://mathai2023.github.io/" target="_blank">[MATH-AI]</a>
						</li>
					</ul>
					<p>
						In addition, I am interested in:
					</p>
					<ul>
						<li>
							<strong>Parameter-efficient instruction tuning</strong> for large language models and large multimodal models
							<a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank">[ICLR-24]</a> 
							<a href="https://arxiv.org/abs/2303.16199" target="_blank">[LLaMA-Adapter V2]</a>
						</li>
						<li>
							<strong>Conversational agents</strong> that are socially intelligent and human-in-loop, understanding emotions, relationships, values, and theory of mind 
							<a href="https://chats-lab.github.io/KokoMind/" target="_blank">[KokoMind]</a>
							<a href="https://arxiv.org/abs/2103.07011" target="_blank">[SIGDIAL-22]</a>
							<a href="https://arxiv.org/abs/2112.06346" target="_blank">[AAAI-22]</a>
							<a href="https://arxiv.org/abs/2106.01006" target="_blank">[ACL-21]</a>
						</li>
						<li>
							<strong>Multimodal reasoning</strong> for vision, language, and knowledge
							<a href="https://lupantech.github.io/papers/aaai20_caption.pdf" target="_blank">[AAAI-20] 
							<a href="https://lupantech.github.io/papers/ijcai19_matching.pdf" target="_blank">[IJCAI-19]</a>
							<a href="https://lupantech.github.io/papers/cvpr19_dynamicvqa.pdf" target="_blank">[CVPR-19]</a> 
							<a href="https://lupantech.github.io/papers/eccv18_hybridvqa.pdf" target="_blank">[ECCV-18]</a> 
							<a href="https://lupantech.github.io/papers/kdd18_rvqa.pdf" target="_blank">[KDD-18]</a> 
							<a href="https://lupantech.github.io/papers/aaa18_dualvqa.pdf" target="_blank">[AAAI-18]</a>
						</li>
					</ul>
				</div>
			</div>
		</div>
	</div>
	<br>

	<div class="container">
		<div class="container" id="div-twitter">
			<a class="twitter-timeline" id="twitter" data-width=50% data-height=650 data-theme=light
				href="https://twitter.com/lupantech?ref_src=twsrc%5Etfw">Tweets by lupantech</a>
			<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
		</div>
		<div class="container" id="div-news" >
			<h3 id="News" style="">News</h3>
			<hr>
			<ul>
				<li>
					<strong>[03/2024]&nbsp; <font color="red">New!</font></strong>
					I am co-organizing the <b><a href="https://sites.google.com/view/ai4mathworkshopicml2024" target="_blank">AI for Math</a></b>
					Workshop at <b><a href="https://icml.cc/Conferences/2024" target="_blank">ICML 2024</a></b>. See you in Vienna!
				</li>
				<li>
					<strong>[03/2024]&nbsp; <font color="red">New!</font></strong>
					A paper on visual math reasoning with Multi-modal LLMs is available at
					<b><a href="https://arxiv.org/abs/2403.14624" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[02/2024]&nbsp; <font color="red">New!</font></strong>
					A paper on LLMs for advanced quantitative reasoning is available at
					<b><a href="https://arxiv.org/abs/2402.17644" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[01/2024]&nbsp; <font color="red">New!</font></strong>
					Two papers on large multimodal models are accepted to
					<b><a href="https://iclr.cc/Conferences/2024" target="_blank">ICLR 2024</a></b>.
				</li>
				<li>
					<strong>[01/2024]&nbsp; <font color="red">New!</font></strong>
					A paper on model editing for LLMs is available at
					<b><a href="https://arxiv.org/abs/2401.04700" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[12/2023]&nbsp; <font color="red">New!</font></strong>
					I am co-organizing the <b><a href="https://sites.google.com/view/tavi-cvpr24/" target="_blank">Tool-Augmented VIsion</a></b>
					Workshop at <b><a href="https://cvpr.thecvf.com/" target="_blank">CVPR 2024</a></b>. See you in Seattle!
				</li>
				<li>
					<strong>[12/2023]&nbsp; <font color="red">New!</font></strong>
					I am attending <b><a href="https://nips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b> from Dec 10 to Dec 16. See you in New Orleans!
				</li>
				<li>
					<strong>[12/2023]&nbsp; <font color="red">New!</font></strong>
					Google's <b><a href="https://blog.google/technology/ai/google-gemini-ai/?utm_source=twitter&utm_medium=social&utm_campaign=GDMGemini#performance" target="_blank">Gemini</a></b> benchmarks our <b><a href="https://arxiv.org/abs/2310.02255" target="_blank">MathVista</a></b> for evaluating math reasoning in visual contexts!
				</li>
				<li>
					<strong>[11/2023]&nbsp; <font color="red">New!</font></strong>
					Honored to be covered by <b><a href="https://www.cs.ucla.edu/phd-student-pan-lu-wins-2023-qualcomm-innovation-fellowship/" target="_blank">UCLA CS</a></b> for winning <b><a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america" target="_blank">Qualcomm Innovation Fellowship</a></b>. Thanks!
				</li>
				<li>
					<strong>[10/2023]&nbsp; <font color="red">New!</font></strong>
					The 112-page study on GPT-4V, Bard, and others on visual math reasoning is available
					<b><a href="https://arxiv.org/abs/2310.02255" target="_blank">here</a></b>.
				</li>
				<li>
					<strong>[10/2023]&nbsp; <font color="red">New!</font></strong>
					Honored to serve as PC Chair and co-organize
					<b><a href="https://socalnlp.github.io/symp23/index.html" target="_blank">SoCal NLP 2023</a></b>. See you in LA!
				</li>
				<li>
					<strong>[10/2023]&nbsp; <font color="red">New!</font></strong>
					One paper on mathematical reasoning is accepted to
					<b><a href="https://2023.emnlp.org/" target="_blank">EMNLP 2023</a></b>.
				</li>
				<li>
					<strong>[10/2023]&nbsp; <font color="red">New!</font></strong>
					One paper on mathematical reasoning in visual contexts
					(<b><a href="https://arxiv.org/abs/2310.02255" target="_blank">MathVista</a></b>)
					is submitted to <b>Preprint</b>.
				</li>
				<li>
					<strong>[09/2023]&nbsp; <font color="red">New!</font></strong>
					One paper on tool-augmented LLMs is accepted to
					<b><a href="https://nips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b>.
				</li>
				<li>
					<strong>[07/2023]&nbsp; <font color="red">New!</font></strong>
					One paper on a scientific reasoning benchmark
					(<b><a href="https://arxiv.org/abs/2307.10635" target="_blank">SciBench</a></b>)
					is submitted to <b>Preprint</b>.
				</li>
				<li>
					<strong>[07/2023]&nbsp; <font color="red">New!</font></strong>
					I am co-organizing the 3rd <b><a href="https://mathai2023.github.io/" target="_blank">MATH-AI</a></b>
					Workshop at <b>NeurIPS 2023</b>. See you in New Orleans!
				</li>
				<li>
					<strong>[06/2023]&nbsp; <font color="red">New!</font></strong>
					 Excited to receive the <b><a href="https://grad.ucla.edu/funding/financial-aid/funding-for-continuing-students/dissertation-year-fellowship/" target="_blank"> UCLA Dissertation Year Fellowship</a></b>. 
				</li>
				<li>
					<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
					Honored to deliver a guest lecture for UCLA CS 263: Natural Language Processing. 
					<b><a href="docs/UCLA CS263 NLP_Guest Lecture_Pan Lu_2023.05.31.pdf" target="_blank">[Slides]</a></b>
				</li>
				<li>
					<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
					One paper on theorem-driven math question answering
					(<b><a href="https://github.com/wenhuchen/TheoremQA" target="_blank">TheoremQA</a></b>)
					is available at
					<b><a href="https://arxiv.org/abs/2305.12524" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
					Honored to deliver a invited talk on tool-augmented LLMs at Google Brain. 
					<b><a href="docs/Chameleon_LLM_Pan_Lu_Google_Brain_2023.05.05.pdf" target="_blank">[Slides]</a></b>
				</li>
				<li>
					<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
					Delighted to join prestigious <b><a href="https://lightning.ai/" target="_blank">LightingAI</a></b>
					event as invited speaker on Discord. 
				</li>
				<li>
					<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
					A paper on multimodal procedural planning is available at
					<b><a href="https://arxiv.org/abs/2305.01795" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[05/2023]&nbsp;  <font color="red">New!</font></strong>
					One survey paper on deep learning for mathematical reasoning is accepted to <b>ACL 2023</b></font></b>.
				</li>
				<li>
					<strong>[04/2023]&nbsp; <font color="red">New!</font></strong>
					<b><a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank">LLaMA-Adapter-V2</a></b>, 
					a parameter-efficient visual instruction model</b>, 
					is available at
					<b><a href="https://arxiv.org/abs/2304.15010" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[04/2023]&nbsp; <font color="red">New!</font></strong>
					One tutorial proposal on mathematical reasoning is accepted to
					<b><a href="https://ijcai-23.org/" target="_blank">IJCAI 2023</a></b>.
				</li>
				<li>
					<strong>[04/2023]&nbsp; <font color="red">New!</font></strong>
					One paper on tool augmented LLMs
					(<b><a href="https://chameleon-llm.github.io/" target="_blank">Chameleon</a></b>)
					is available at
					<b><a href="https://arxiv.org/abs/2304.09842" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[04/2023]&nbsp; <font color="red">New!</font></strong>
					Two papers are accepted to
					<b><a href="https://asu-apg.github.io/odrum/" target="_blank">CVPR 2023 O-DRUM Workshop</a></b>.
					<!-- as an <b><font color="red">Oral Presentation</font></b> -->
				</li>
				<li>
					<strong>[03/2023]&nbsp; <font color="red">New!</font></strong>
					One paper on fine-tuning
					<b><a href="https://github.com/facebookresearch/llama" target="_blank">LLaMA</a></b>
					in one hour
					(<b><a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank">LLaMA-Adapter</a></b>)
					is available at
					<b><a href="https://arxiv.org/abs/2303.16199" target="_blank">Preprint</a></b>.
				</li>
			</ul>
			<div id="content"> 
				<span id="dots"></span>
				<span id="more">
					<ul style="margin-top: -15px">
						<li>
							<strong>[01/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on in-context learning for math reasoning
							(<b><a href="https://tabmwp.github.io/" target="_blank">PromptPG</a></b>)
							is accepted to <b>ICLR 2023</b>.
						</li>
						<li>
							<strong>[12/2022]&nbsp; <font color="red">New!</font></strong>
							A survey paper on deep learning for mathematical reasoning is available at
							<b><a href="https://arxiv.org/abs/2212.10535" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[12/2022]&nbsp; <font color="red">New!</font></strong>
							One paper is accepted to
							<b><a href="https://knowledge-nlp.github.io/aaai2023/publications.html" target="_blank">AAAIâ€™23 KnowledgeNLP Workshop</a></b> 
							as an <b><font color="red">Oral Presentation</font></b>.
						</li>
						<li>
							<strong>[12/2022]&nbsp; <font color="red">New!</font></strong>
							I am excited to join <b><a href="https://www.microsoft.com/en-us/research/" target="_blank">Microsoft Research</a></b>
							as a research intern!
						</li>
						<li>
							<strong>[10/2022]&nbsp; <font color="red">New!</font></strong>
							Happy to receive the <b>NeurIPS 2022 Scholar Award</b>.
						</li>
						<li>
							<strong>[10/2022]&nbsp; <font color="red">New!</font></strong>
							Two papers on mathematical reasoning are accepted to <b>EMNLP 2022</b>.
						</li>
						<li>
							<strong>[09/2022]&nbsp; <font color="red">New!</font></strong>
							One paper on prompt learning for math reasoning
							(<b><a href="https://tabmwp.github.io/" target="_blank">PromptPG</a></b>)
							is submitted to <b>Preprint</b>.
						</li>
						<li>
							<strong>[09/2022]&nbsp; <font color="red">New!</font></strong>
							One paper on chain-of-thought reasoning for
							<b><a href="https://scienceqa.github.io/" target="_blank">ScienceQA</a></b>
							is accepted to <b>NeurIPS 2022</b>.
						</li>
						<li>
							<strong>[07/2022]&nbsp; <font color="red">New!</font></strong>
							I am co-organizing the 2nd <b><a href="https://mathai2022.github.io/" target="_blank">MATH-AI</a></b>
							Workshop at <b>NeurIPS 2022</b>. See you in New Orleans!
						</li>
						<li>
							<strong>[07/2022]&nbsp; <font color="red">New!</font></strong>
							One paper on socially intelligent agents is accepted to <b>SIGDIAL 2022</b>.
						</li>
						<li>
							<strong>[04/2022]&nbsp;</strong>
							Excited to be listed as a
							<b><a href="https://iclr.cc/Conferences/2022/Reviewers" target="_blank">Highlighted Reviewer</a></b>
							for ICLR 2022.
						</li>
						<li>
						<strong>[03/2022]&nbsp;</strong>  
						I am excited to join <b><a href="https://allenai.org/" target="_blank">Allen Institute for AI (AI2)</a></b>
						as a research intern!
						</li>
						<li>
							<strong>[03/2022]&nbsp;</strong>
							One paper on character animation sampling is submitted to <b>Preprint</b>.
						</li>
						<!-- <li>
							<strong>[02/2022]&nbsp;</strong>
							Data and code for NeurIPS 2021
							<b><a href="https://iconqa.github.io/" target="_blank">IconQA</a></b>
							are released now!
						</li> -->
						<li>
							<strong>[12/2021]&nbsp;</strong>
							Two papers are accepted to <b>AAAI 2022</b></font></b>.
						</li>
						<li>
							<strong>[10/2021]&nbsp;</strong>
							One paper on visual question answering for icon images
							(<b><a href="https://iconqa.github.io/" target="_blank">IconQA</a></b>)
							is accepted to <b>NeurIPS 2021</b>.
						</li>
						<li>
							<strong>[07/2021]&nbsp;</strong>
							I am co-organizing the <b><a href="https://mathai4ed.github.io/" target="_blank">MATHAI4ED</a></b>
							Workshop at <b>NeurIPS 2021</b>. Welcome to participate!
						</li>
						<li>
							<strong>[07/2021]&nbsp; </strong>
							Our workshop proposal for Math AI for Education (MATHAI4ED) is accepted to <b>NeurIPS 2021</b></font>.
							</b>
						</li>
						<li>
							<strong>[05/2021]&nbsp; </strong>
							One paper on interpretable geometry problem solving is accepted to <b>ACL 2021</b> as an <b>
								<font color="red">Oral Presentation</font>.
							</b>
						</li>
						<li>
							<strong>[05/2021]&nbsp; </font>
							</strong> One paper on social relation inference in dialogues is accepted to <b>ACL 2021</b> as an <b>
								<font color="red">Oral Presentation</font>.
							</b>
						</li>
						<li>
							<strong>[03/2021]</strong> One paper on socially intelligent agents is submitted to <b>Preprint</b>.
						</li>
						<!-- <li>
							<strong>[11/2019]</strong> One paper on personalized image caption is accepted to <b>AAAI 2020</b>.
						</li>
						<li>
							<strong>[05/2019]</strong> One paper on knowledge aware image-text matching is accepted to <b>IJCAI
								2019</b> as an <b>
								<font color="red">Oral Presentation</font>.
							</b>
						</li>
						<li>
							<strong>[03/2019]</strong> One paper on dynamic fusion for visual question answering is accepted to
							<b>CVPR 2019</b> as an <b>
								<font color="red">Oral Presentation</font>.
							</b>
						</li> -->
					</ul>
				</span>
			</div>
			<button onclick="showMore()" id="showMoreBtn">Show more</button>
		</div>
	</div>
	<br>

	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="">Selected Publications</h3>
		<hr>
		<!-- <font color="black">(* indicate equal contribution)</font><br><hr> -->

		<!-- MathVista -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/iclr24_mathvista.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://mathvista.github.io/">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</font>
				</b><br>
				<b>Pan Lu</b>, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao
				<br>
				<b><a href="https://arxiv.org/abs/2310.02255" target="_blank">ICLR 2024&nbsp;</a></b>
				<a href="https://mathvista.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2310.02255" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/iclr24_mathvista.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/lupantech/MathVista" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/AI4Math/MathVista" target="_blank"> <small>[Dataset]&nbsp;</small></a>
				<a href="https://mathvista.github.io/#leaderboard" target="_blank"> <small>[Leaderboard]&nbsp;</small></a>
				<a href="https://mathvista.github.io/#visualization" target="_blank"> <small>[Visualize]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1717313355780964608" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/iclr24_mathvista.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<a id="best" href="" target="_blank">
					<font color="red"><b>Oral Presentation</b> </font>
				</a> (85 in 7304 submissions, 1.2%) <br>
				<a id="best" href="https://cryptorank.io/news/feed/d1046-ais-roblem-solving-examined-with-mathvista" target="_blank">
					<font color="red"><b>CryptoRank News Feature</b> </font>
				</a> (29 October 2023) <br>
			</div>
		</div>
		<hr>

		<!-- MathVerse -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv24_mathverse.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
					</font>
				</b><br>
				Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, <b>Pan Lu</b>, Kai-Wei Chang, Peng Gao, Hongsheng Li
				<br>
				<b><a href="https://arxiv.org/abs/2403.14624" target="_blank">arXiv:2403.14624&nbsp;</a></b>
				<a href="https://mathverse-cuhk.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2403.14624" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2403.14624.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/ZrrSkywalker/MathVerse" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/AI4Math/MathVerse" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://mathverse-cuhk.github.io/#visualization" target="_blank"> <small>[Visualization]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1771019526265618889" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="https://huggingface.co/papers/2403.14624" target="_blank"> <small>[Daily Papers]&nbsp;</small></a>
				<a href="bibs/arxiv24_mathverse.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- QRData -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv24_qrdata.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data
					</font>
				</b><br>
				Xiao Liu, Zirui Wu, Xueqing Wu, <b>Pan Lu</b>, Kai-Wei Chang, Yansong Feng
				<br>
				<b><a href="https://arxiv.org/abs/2402.17644" target="_blank">arXiv:2402.17644&nbsp;</a></b>
				<a href="https://xxxiaol.github.io/QRData/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2402.17644" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2402.17644.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/xxxiaol/QRData" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://github.com/xxxiaol/QRData" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://twitter.com/xxxxiaol/status/1763061050868674890" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/arxiv24_qrdata.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- SPHINX-X -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv24_sphinx.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://mathvista.github.io/">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
					</font>
				</b><br>
				Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, <b>Pan Lu</b>, Hongsheng Li, Yu Qiao
				<br>
				<b><a href="https://arxiv.org/abs/2402.05935" target="_blank">arXiv:2402.05935&nbsp;</a></b>
				<!-- <a href="https://mathvista.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a> -->
				<a href="https://arxiv.org/abs/2402.05935" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2402.05935.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://llama2-accessory.readthedocs.io/en/latest/" target="_blank"> <small>[Doc]&nbsp;</small></a>
				<a href="https://huggingface.co/papers/2402.05935" target="_blank"> <small>[Hugging Face]&nbsp;</small></a>
				<a href="" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1755794618019368978" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/arxiv24_sphinx.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>
		
		<!-- Model Editing -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv24_modelediting.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://mathvista.github.io/">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">Model Editing Can Hurt General Abilities of Large Language Models</font>
				</b><br>
				Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, <b>Pan Lu</b>, Zhen-Hua Ling, Kai-Wei Chang, Nanyun Peng
				<br>
				<b><a href="https://arxiv.org/abs/2401.04700" target="_blank">arXiv:2401.04700&nbsp;</a></b>
				<!-- <a href="https://mathvista.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a> -->
				<a href="https://arxiv.org/abs/2401.04700" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2401.04700.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/JasonForJoy/Model-Editing-Hurt" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/JasonForJoy/status/1744932437006885060" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<!-- <a href="" target="_blank"> <small>[Coverage]&nbsp;</small></a> -->
				<a href="bibs/arxiv24_modelediting.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- LLaMA-Adapter -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/iclr24_llama.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention					</font>
				</b><br>
				Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, <b>Pan Lu</b>, Hongsheng Li, Peng Gao, Yu Qiao
				<br>
				<b><a href="https://arxiv.org/abs/2303.16199" target="_blank">ICLR 2024&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2303.16199" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/iclr24_llama.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1640899600281395200" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1640884275439628288" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/iclr24_llama.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<a id="best" href="https://lightning.ai/pages/community/article/understanding-llama-adapters/" target="_blank">
					<font color="red"><b>LightningAI Blog Feature</b> </font>
				</a> (14 April 2023) <br>
			</div>
		</div>
		<hr>

		<!-- Chameleon -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips23_chameleon.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://chameleon-llm.github.io/">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</font>
				</b><br>
				<b>Pan Lu</b>, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao
				<br>
				<b><a href="https://arxiv.org/abs/2304.09842" target="_blank">NeurIPS 2023&nbsp;</a></b>
				<a href="https://chameleon-llm.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2304.09842" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/neurips23_chameleon.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/lupantech/chameleon-llm" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1648879085115052033" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1648851856930533378" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/neurips23_chameleon.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<a id="best" href="https://alphasignalai.beehiiv.com/p/weeks-top-5-ai-papers?utm_source=alphasignalai.beehiiv.com&utm_medium=newsletter&utm_campaign=this-week-s-top-5-ai-papers" target="_blank">
					<font color="red"><b>Best Weekly AI Paper</b> </font>
				</a> (by AlphaSignal, 1st in 1682, 0.06%) <br>
				<a id="best" href="https://github.com/jacobmarks/awesome-neurips-2023/tree/main" target="_blank">
					<font color="red"><b>Awesome NeurIPS 2023 Papers</b> </font>
				</a> (40 in 3584, 0.01%) <br>
				<a id="best" href="https://medium.com/voxel51/neurips-2023-survival-guide-2f957d5b07c9" target="_blank">
					<font color="red"><b>NeurIPS 2023 Top 10 Multimodal ML Papers </b> </font>
				</a>

			</div>
		</div>
		<hr>

		<!-- SciBench -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv23_scibench.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</font>
				</b><br>
				Xiaoxuan Wang*, Ziniu Hu*, <strong>Pan Lu</strong>*, Yanqiao Zhu*, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang
				<br>
				<b><a href="https://arxiv.org/abs/2307.10635" target="_blank">arXiv:2305.00970&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2307.10635" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/arxiv23_scibench.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/mandyyyyii/scibench" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/arxiv23_scibench.txt" target="_blank"> <small>[BibTex]</small></a><br>
				(*Equal Contribution)<br>
				<a id="best" href="https://www.nature.com/articles/d41586-023-03507-3" target="_blank">
					<font color="red"><b>Nature News Feature</b> </font>
				</a>
				(15 November 2023)
			</div>
		</div>
		<hr>

		<!-- KokoMind -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/kokomind2023.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://chats-lab.github.io/KokoMind/">
					<img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">KokoMind: Can LLMs Understand Social Interactions?</font>
				</b><br>
				Weiyan Shi*, Liang Qiu*, Dehong Xu, Pengwei Sui, <b>Pan Lu</b>, Zhou Yu<br>
				<a href="https://chats-lab.github.io/KokoMind/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/CHATS-lab/KokoMind/tree/main" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1676708418697203713" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/shi_weiyan/status/1676697071674601473" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/kokomind2023.txt" target="_blank"> <small>[BibTex]</small></a><br>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- TheoremQA -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp23_theoremqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://github.com/wenhuchen/TheoremQA">
					<img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">TheoremQA: A Theorem-driven Question Answering Dataset</font>
				</b><br>
				Wenhu Chen, Ming Yin, Max Ku, <b>Pan Lu</b>, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia<br>
				<b><a href="https://2023.emnlp.org/" target="_blank">EMNLP 2023&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2305.12524" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/emnlp23_theoremqa.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/wenhuchen/TheoremQA" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/WenhuChen/status/1660832837715611648" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/emnlp23_theoremqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- Ark -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv23_ark.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="">
					<img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">ArK: Augmented Reality with Knowledge Emergent Infrastructure</font>
				</b><br>
				Abhinav Gupta*, Qiuyuan Huang*, Jae Sung Park*, <strong>Pan Lu</strong>*, Paul N. Bennett, Ran Gong, Subhojit Som, Baolin Peng, Owais Khan Mohammed, Christopher Pal, Yejin Choi, Jianfeng Gao<br>
				<b><a href="https://arxiv.org/abs/2305.00970" target="_blank">arXiv:2305.00970&nbsp;</a></b>
				<a href="papers/arxiv23_ark.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/04/ArK.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/arxiv23_ark.txt" target="_blank"> <small>[BibTex]</small></a><br>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- MPP -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv23_mpp.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://github.com/YujieLu10/TIP">
					<img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">Multimodal Procedural Planning via Dual Text-Image Prompting</font>
				</b><br>
				Yujie Lu, <b>Pan Lu</b>, Zhiyu Chen, Wanrong Zhu, Xin Eric Wang, William Yang Wang<br>
				<b><a href="https://arxiv.org/abs/2305.01795" target="_blank">arXiv:2305.01795&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2305.01795" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/arxiv23_mpp.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/YujieLu10/TIP" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/yujielu_10/status/1653928324408958977" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1653922454933319680" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/arxiv23_mpp.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- LLaMA-Adapter-V2 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv23_llamav2.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</font>
				</b><br>
				Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, <b>Pan Lu</b>, Conghui He, Xiangyu Yue, Hongsheng Li, Yu Qiao
				<br>
				<b><a href="https://arxiv.org/abs/2304.15010" target="_blank">arXiv:2304.15010&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2304.15010" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/arxiv23_llamav2.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="http://llama-adapter.opengvlab.com/" target="_blank"> <small>[Gradio]&nbsp;</small></a>
				<a href="http://imagebind-llm.opengvlab.com/" target="_blank"> <small>[Gradio-Multimodal]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1652022897563795456" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://www.youtube.com/watch?v=GAJyWkkSd8M&ab_channel=BruinTech" target="_blank"> <small>[YouTube]&nbsp;</small></a>
				<!-- <a href="https://twitter.com/_akhaliq/status/1640884275439628288" target="_blank"> <small>[Coverage]&nbsp;</small></a> -->
				<a href="bibs/arxiv23_llamav2.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- Survey -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl23_dl4math.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://github.com/lupantech/dl4math">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">A Survey of Deep Learning for Mathematical Reasoning</font>
				</b><br>
				<b>Pan Lu</b>, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang<br>
				<b><a href="https://arxiv.org/abs/2212.10535" target="_blank">ACL 2023&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2212.10535" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/acl23_dl4math.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/lupantech/dl4math" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="posters/acl23_dl4math.pdf" target="_blank"> <small>[Poster]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1605400505697841155" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/papers_daily/status/1607232372289724416" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/acl23_dl4math.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- TabMWP -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/iclr23_promptpg.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://promptpg.github.io/">
					<!-- <img src="logos/promptpg.png" id="logo"> -->
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical
						Reasoning
					</font>
				</b><br>
				<b>Pan Lu</b>, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark,
				Ashwin Kalyan<br>
				<b><a href="https://arxiv.org/abs/2209.14610" target="_blank">ICLR 2023&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2209.14610" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/iclr23_promptpg.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://promptpg.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://drive.google.com/drive/folders/1IYgGrY9agwF_qQlh4WNRG_4WF_ggTkvG?usp=sharing"
					target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://github.com/lupantech/PromptPG" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://promptpg.github.io/explore.html" target="_blank"> <small>[Explore]&nbsp;</small></a>
				<a href="https://promptpg.github.io/leaderboard.html" target="_blank">
					<small>[Leaderboard]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1623039527026831361" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/iclr23_promptpg.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- ScienceQA -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips22_scienceqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://scienceqa.github.io/">
					<!-- <img src="logos/scienceqa.png" id="logo"> -->
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question
						Answering</font>
				</b><br>
				<b>Pan Lu</b>, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
				Clark,
				Ashwin Kalyan<br>
				<b><a href="https://nips.cc/" target="_blank">NeurIPS 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2209.09513" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/neurips22_scienceqa.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://scienceqa.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://drive.google.com/drive/u/1/folders/1w8imCXWYn2LxajmGeGH_g5DaL2rabHev" target="_blank">
					<small>[Data]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/derek-thomas/ScienceQA" target="_blank"> <small>[Huggingface]&nbsp;</small></a>
				<a href="https://github.com/lupantech/ScienceQA" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://scienceqa.github.io/explore.html" target="_blank"> <small>[Explore]&nbsp;</small></a>
				<a href="https://scienceqa.github.io/leaderboard.html" target="_blank">
					<small>[Leaderboard]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1570828580346802178" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/neurips22_scienceqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- Lila -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp22_lila.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://lila.apps.allenai.org/" style="font-size:20pt">
					ðŸ“œ
				</a> -->
				<b>
					<font color="black">LILA: A Unified Benchmark for Mathematical Reasoning</font>
				</b><br>
				Swaroop Mishra*, Matthew Finlayson*, <b>Pan Lu</b>, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay
				Rajpurohit,
				Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin K. Kalyan
				<br>
				<b><a href="https://2022.emnlp.org/" target="_blank">EMNLP 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2210.17517" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2210.17517.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://lila.apps.allenai.org/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/allenai/Lila" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://github.com/allenai/Lila" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/allenai/bhaskara" target="_blank">
					<small>[Huggingface]&nbsp;</small></a>
				<a href="bibs/emnlp22_lila.txt" target="_blank"> <small>[BibTex]</small></a><br>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- UniGeo -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp22_unigeo.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical
						Expression</font>
				</b><br>
				Jiaqi Chen, Tong Li, Jinghui Qin, <b>Pan Lu</b>, Liang Lin, Chongyu Chen and Xiaodan Liang
				<br>
				<b><a href="https://2022.emnlp.org/" target="_blank">EMNLP 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2212.02746" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/emnlp22_unigeo.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/chen-judge/UniGeo" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/emnlp22_unigeo.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- Mind -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv21_mind.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Towards Socially Intelligent Agents with Mental State Transition and Human
						Utility</font>
				</b><br>
				<a>Liang Qiu*, Yizhou Zhao*, Yuan Liang, <b>Pan Lu</b>, Weiyan Shi, Zhou Yu,
					Song-Chun Zhu</a><br>
				<b><a href="https://2022.sigdial.org/" target="_blank">SIGDIAL 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2103.07011" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/sigdial22_mind.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/sigdial22_mind.txt" target="_blank"> <small>[BibTex]</small></a><br>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- Animation -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv22_animation.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Triangular Character Animation Sampling with Motion, Emotion, and Relation
					</font>
				</b><br>
				<a>Yizhou Zhao, Liang Qiu, Wensi Ai, <b>Pan Lu</b>, Song-Chun Zhu</a><br>
				<b><a href="https://arxiv.org/abs/2203.04930" target="_blank">arXiv:2203.04930&nbsp;</a></b>
				<a href="https://arxiv.org/pdf/2203.04930.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/arxiv22_animation.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- AAAI22 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai22_tangram.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Learning from the Tangram to Solve Mini Visual Tasks</font>
				</b><br>
				<a>Yizhou Zhao, Liang Qiu, <b>Pan Lu</b>, Feng Shi, Tian Han, Song-Chun Zhu</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-22/" target="_blank">AAAI 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2112.06113" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2112.06113.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<!-- <a href="" target="_blank"> <small>[Project]&nbsp;</small></a> -->
				<a href="https://github.com/yizhouzhao/Tangram" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/aaai22_tangram.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI22 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai22_valuenet.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">ValueNet: A New Dataset for Human Value Driven Dialogue System</font>
				</b><br>
				<a>Liang Qiu, Yizhou Zhao, Jinchao Li, <b>Pan Lu</b>, Baolin Peng, Jianfeng Gao, Song-Chun Zhu</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-22/" target="_blank">AAAI 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2112.06346" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/aaai22_valuenet.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://liang-qiu.github.io/ValueNet/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<!-- <a href="" target="_blank"> <small>[Code]&nbsp;</small></a> -->
				<a href="bibs/aaai22_valuenet.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<!-- <font color="firebrick"><b>Oral Presentation</b></font> -->
			</div>
		</div>
		<hr>

		<!-- GenMotion -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv21_genmotion.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">GenMotion: Data-driven Motion Generators for Real-time Animation Synthesis
					</font>
				</b><br>
				<a>Yizhou Zhao, Wensi Ai, Liang Qiu, <b>Pan Lu</b>, Feng Shi, Tian Han, Song-Chun Zhu</a><br>
				<b><a href="https://arxiv.org/abs/2112.06060" target="_blank">arXiv:2112.06060&nbsp;</a></b>
				<a href="https://arxiv.org/pdf/2112.06060.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/arxiv21_genmotion.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- NeurIPS21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips21_iconqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://iconqa.github.io/">
					<img src="logos/iconqa.png" id="logo">
				</a> -->
				<b>
					<font color="black">IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language
						Reasoning</font>
				</b><br>
				<a><b>Pan Lu</b>, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang,
					Song-Chun Zhu</a><br>
				<b><a href="https://nips.cc/Conferences/2021" target="_blank">NeurIPS 2021&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2110.13214" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/neurips21_iconqa.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://iconqa.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/lupantech/IconQA" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/neurips21_iconqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Datasets and Benchmarks Track</b></font>
			</div>
		</div>
		<hr>

		<!-- ACL21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl21_gps.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://lupantech.github.io/inter-gps/">
					<img src="logos/intergps.svg" id="logo">
				</a> -->
				<a href="https://lupantech.github.io/inter-gps/">
					<!-- <img src="logos/scienceqa.png" id="logo"> -->
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and
						Symbolic Reasoning</font>
				</b><br>
				<a><b>Pan Lu</b>*, Ran Gong*, Shibiao Jiang*, Liang Qiu, Siyuan Huang,
					Xiaodan Liang, Song-Chun Zhu</a><br>
				<b><a href="https://2021.aclweb.org/" target="_blank">ACL 2021&nbsp;</a></b>
				<a href="https://aclanthology.org/2021.acl-long.528/" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/acl21_intergps.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://lupantech.github.io/inter-gps/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/lupantech/InterGPS" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/acl21_gps.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- ACL21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl21_soc.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues
					</font>
				</b><br>
				<a>Liang Qiu, Yuan Liang, Yizhou Zhao, <b>Pan Lu</b>, Baolin Peng, Zhou Yu, Ying Nian Wu, Song-Chun
					Zhu</a><br>
				<b><a href="https://2021.aclweb.org/" target="_blank">ACL 2021&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2106.01006" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/acl21_soc.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/acl21_soc.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI20 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai20_caption.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Learning Long- and Short-Term User Literal-Preference with Multimodal
						Hierarchical Transformer Network for Personalized Image Caption</font>
				</b><br>
				<a>Wei Zhang, Yue Ying, <strong>Pan Lu</strong>, Hongyuan Zha</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-20/" target="_blank">AAAI 2020&nbsp;</a></b>
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6503" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/aaai20_caption.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/aaai20_caption.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- IJCAI19 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/ijcai19_matching.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Knowledge Aware Semantic Concept Expansion for Image-Text Matching</font>
				</b><br>
				<a>Botian Shi, Lei Ji, <strong>Pan Lu</strong>, Nan Duan</a><br>
				<b><a href="https://ijcai19.org/" target="_blank">IJCAI 2019&nbsp;</a></b>
				<a href="papers/ijcai19_matching.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/ijcai19_matching.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- CVPR19 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/cvpr19_dynamicvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Dynamic Fusion with Intra- and Inter-modality Attention Flow for Visual Question
						Answering</font>
				</b><br>
				<a>Peng Gao, Zhengkai Jiang, Haoxuan You, <strong>Pan Lu</strong>, Steven CH Hoi, Xiaogang Wang,
					Hongsheng Li</a><br>
				<b><a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019&nbsp;</a></b>
				<a href="papers/cvpr19_dynamicvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/bupt-cist/DFAF-for-VQA.pytorch" target="_blank">
					<small>[Code]&nbsp;</small></a>
				<a href="bibs/cvpr19_dynamicvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- PAKDD19 -->
		<!-- <div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/pakdd2019_hybrid.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b><font color="black">A Novel Hybrid Sequential Model for Review-based Rating Prediction</font></b><br>
				<a>Yuanquan Lu, Wei Zhang, <strong>Pan Lu</strong>, Jianyong Wang</a><br>
				<b><a href="https://pakdd2019.medmeeting.org/en" target="_blank">PAKDD 2019&nbsp;</a></b>
				<a href="papers/pakdd2019_hybrid.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/pakdd2019_hybrid.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div><hr> -->

		<!-- ICDE19  -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/icde19_knowledge.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Knowledge-Aware Deep Dual Networks for Text-Based Mortality Prediction</font>
				</b><br>
				<a>Ning Liu, <strong>Pan Lu</strong>, Wei Zhang, Jianyong Wang</a><br>
				<b><a href="http://conferences.cis.umac.mo/icde2019/" target="_blank">ICDE 2019&nbsp;</a></b>
				<a href="papers/icde19_knowledge.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/icde19_knowledge.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- ECCV18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/eccv18_hybridvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Question-Guided Hybrid Convolution for Visual Question Answering</font>
				</b><br>
				<a>Peng Gao, Hongsheng Li, Shuang Li, <strong>Pan Lu</strong>, Yikang Li, Steven Hoi, Xiaogang
					Wang</a><br>
				<b><a href="https://eccv2018.org/" target="_blank">ECCV 2018&nbsp;</a></b>
				<a href="papers/eccv18_hybridvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/eccv18_hybridvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- SIGKDD18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/kdd18_rvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual
						Question Answering</font>
				</b><br>
				<a><strong>Pan Lu</strong>, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, Jianyong Wang</a><br>
				<b><a href="https://www.kdd.org/kdd2018/" target="_blank">SIGKDD 2018&nbsp;</a></b>
				<a href="papers/kdd18_rvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/lupantech/rvqa" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://www.youtube.com/watch?v=OQoqpRuY4L4" target="_blank"> <small>[Video]&nbsp;</small></a>
				<a href="bibs/kdd18_rvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaa18_dualvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Co-attending Free-form Regions and Detections with Multi-modal Multiplicative
						Feature Embedding for Visual Question Answering</font>
				</b><br>
				<a><strong>Pan Lu</strong>, Hongsheng Li, Wei Zhang, Jianyong Wang, Xiaogang Wang</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-18/" target="_blank">AAAI 2018&nbsp;</a></b>
				<a href="papers/aaa18_dualvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/lupantech/dual-mfa-vqa" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/aaa18_dualvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

	</div><br>


	<!-- Education -->
	<div class="container">
		<h3 id="Education" style="">Education</h3>
		<hr>
		<ul>
			<li>
				<strong>University of California, Los Angeles</strong>, 2019 - 2024<br>
				Ph.D. Student in Computer Science
			</li>
			<li>
				<strong>Tsinghua University</strong>, 2015 - 2018<br>
				M.Eng. in Computer Science
			</li>
			<li>
				<strong>Beijing Institute of Technology</strong>, 2011 - 2015<br>
				B.S. in Electrical Engineering (Automation)
			</li>
		</ul>
	</div><br>


	<!-- Experience -->
	<div class="container">
		<h3 id="Experience" style="">Selected Experience</h3>
		<hr>
		<ul>
			<li>
				<strong>Research Intern</strong>, <em>Microsoft Research</em>, 2022.12 - 2023.12 <br>
				Mentors: 
				Dr. <a href="https://sites.google.com/site/hcheng2site" target="_blank">Hao Cheng</a>,
				<!-- Dr. <a href="https://www.microsoft.com/en-us/research/people/bapeng/" target="_blank">Baolin Peng</a>, -->
				Dr. <a href="https://www.microsoft.com/en-us/research/people/mgalley/" target="_blank">Michel Galley</a>,
				Dr. <a href="https://www.microsoft.com/en-us/research/people/jfgao/" target="_blank">Jianfeng Gao</a>
			</li>
			<!-- <li>
				<strong>Graduate Student Researcher</strong>, <em>Center for Vision, Cognition, Learning, and
					Autonomy</em>, <em>UCLA</em>, 2019.9 - Present<br>
				Advisor: Prof. <a href="http://web.cs.ucla.edu/~sczhu/" target="_blank">Song-Chun Zhu</a>
			</li> -->
			<li>
				<strong>Research Intern</strong>, <em>Allen Institute for AI (AI2)</em>, 2022.3 - 2022.12 <br>
				Mentors: Dr. <a href="http://ashwinkalyan.com/" target="_blank">Ashwin Kalyan</a>,
				Dr. <a href="https://allenai.org/team/peterc" target="_blank">Peter Clark</a>
			</li>
			<li>
				<strong>Research Intern</strong>, <em>Document Intelligence Group</em>, <em>Adobe Research</em>, 2021.6
				- 2021.9 <br>
				Mentors: Dr. <a href="https://research.adobe.com/person/jiuxiang-gu/" target="_blank">Jiuxiang Gu</a>,
				Dr. <a href="https://research.adobe.com/person/tong-sun/" target="_blank">Tong Sun</a>
			</li>
			<!-- <li>
				<strong>Researcher Intern</strong>, <em>Basemodel Group</em>, <em>Face Plus</em>, 2019.3 - 2019.7 <br>
				Advisor: Dr. <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en"
					target="_blank">Xiangyu Zhang</a>
			</li> -->
			<li>
				<strong>Research Intern</strong>, <em>Natural Language Computing Group</em>, <em> Microsoft Research
					Asia</em>, 2017.11 - 2018.5<br>
					Mentors: Dr. <a href="https://www.microsoft.com/en-us/research/people/leiji/" target="_blank">Lei
					Ji</a>, Dr. <a href="https://www.microsoft.com/en-us/research/people/nanduan/" target="_blank">Nan
					Duan</a>, Dr. <a href="https://www.microsoft.com/en-us/research/people/mingzhou/"
					target="_blank">Ming Zhou</a>
			</li>
			<li>
				<strong>Research Assistant</strong>, <em>Multimedia Lab</em>, <em>Chinese University of Hong Kong</em>,
				2016.8 - 2017.10 <br>
				Advisors: Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a>, Prof. <a
					href="https://www.ee.cuhk.edu.hk/~xgwang/" target="_blank">Xiaogang Wang</a>
			</li>
			<!-- <li>
				<strong>Research Assistant</strong>, <em>Data Mining Group</em>, <em>Tsinghua University</em>, 2014.11 -
				2018.7 <br>
				Advisor: Prof. <a href="http://dbgroup.cs.tsinghua.edu.cn/wangjy/" target="_blank">Jianyong Wang</a>
			</li> -->
			<!-- <li>
				<strong>Research Intern</strong>, <em>Institute of Automation</em>, <em>Chinese Academy of
					Sciences</em>, 2014.7 - 2014.9 <br>
				Advisor: Prof. <a href="http://www.nlpr.ia.ac.cn/users/szli/" target="_blank">Stan Z. Li</a>
			</li> -->
		</ul>
	</div><br>


	<!-- Teaching -->
	<div class="container">
		<h3 id="Teaching" style="">Teaching</h3>
		<hr>
		<ul>
			<li>
				<strong>Guest Lecturer</strong>, <em>CS 263: Natural Language Processing</em> <a href="docs/UCLA CS263 NLP_Guest Lecture_Pan Lu_2023.05.31.pdf" target="_blank">[Slides]</a> <br>
				Graduate, UCLA, Spring 2023
			</li>
			<li>
				<strong>Teaching Associate</strong>, <em>ECE C247: Neural Networks and Deep Learning</em><br>
				Graduate, UCLA, Winter 2023
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>CS 249: Data Science Fundamentals</em><br>
				Graduate, UCLA, Fall 2022
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>ECE C247: Neural Networks and Deep Learning</em>  <a href="" target="_blank">[Slides]</a> <br>
				Graduate, UCLA, Winter 2022
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>ECE C247: Neural Networks and Deep Learning</em><br>
				Graduate, UCLA, Winter 2022
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>CS 111: Operating System Principles</a></em><br>
				Undergraduate, UCLA, Fall 2021
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>Data Mining: Theory and Algorithms</em><br>
				Graduate, Tsinghua University, Fall 2017
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>C Programming Language</a></em><br>
				Undergraduate, Tsinghua University, Spring 2016
			</li>
		</ul>
	</div><br>


	<!-- Service -->
	<div class="container">
		<h3 id="Service" style="">Professional Service</h3>
		<hr>
		<h4 id="Service" style="">Conferences</h4>
			<ul>
				<li>
					<strong>Program Chair</strong>, 
					<a href="https://socalnlp.github.io/symp23/index.html" target="_blank">The 4th Southern California Natural Language Symposium (SoCal NLP)</a>,
					Los Angeles, 2023.11
				</li>
			</ul>
		<h4 id="Service" style="">Workshops and Tutorials</h4>
			<ul>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://sites.google.com/view/ai4mathworkshopicml2024" target="_blank">AI for Math</a> 
					at ICML 2024
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://sites.google.com/view/tavi-cvpr24/" target="_blank">Tool-Augmented VIsion (TAVI)</a> 
					at CVPR 2024
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://mathai2023.github.io/" target="_blank">MATH-AI: The 3rd Workshop on Mathematical Reasoning and AI</a> 
					at NeurIPS 2023
				</li>
				<li>
					<strong>Co-organizer</strong>, Tutorial on
					<a href="https://ijcai-23.org/" target="_blank">Deep Learning in Mathematical Reasoning: Recent Advances and Beyond</a> 
					at IJCAI 2023
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://mathai2022.github.io/" target="_blank">MATH-AI: Toward Human-Level Mathematical Reasoning</a> 
					at NeurIPS 2022
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://mathai4ed.github.io/" target="_blank">Math AI for Education: Bridging
						the Gap Between Research and Smart Education</a> 
					at NeurIPS 2021
				</li>
			</ul>
			<h4 id="Service" style="">Program Committee Member</h4>
				<ul>
					<li> <strong>2024</strong>: ICLR, ICML, CVPR, COLM, ARR, SIGKDD</li>
					<li> <strong>2023</strong>: NeurIPS, ICLR, ACL, EMNLP, AAAI</li>
					<li> <strong>2022</strong>: NeurIPS, ICLR, CVPR, AAAI,
						COLING</li>
					<li> <strong>2021</strong>: NeurIPS, ICLR, CVPR, ICCV</li>
					<li> <strong>2020 and before</strong>: NeurIPS, AAAI, SIGKDD, ICDM, PAKDD</li>
				</ul>
				<h4 id="Service" style="">Journal Reviewer</h3>
					<ul>
						<li>
							<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34"
								target="_blank">IEEE Transactions on Pattern Analysis and Machine Intelligence</a>
						</li>
						<li>
							<a href="https://www.springer.com/journal/11263/"
								target="_blank">International Journal of Computer Vision</a>
						</li>
						<li>
							<a href="https://www.sciencedirect.com/journal/expert-systems-with-applications"
								target="_blank">Expert Systems With Applications</a>
						</li>
						<li>
							<a href="http://www.ieee-jas.org/" target="_blank">IEEE/CAA JAS</a>,
							<a href="http://www.aas.net.cn/CN/volumn/current.shtml" target="_blank">AAS</a>
						</li>
					</ul>
				<h4 id="Service" style="">Organizations</h3>
					</ul>
						<li> <strong>Chair</strong>,
							IEEE Student Branch at Tsinghua University, Beijing, 2015.10 - 2016.10
						</li>
					</ul>
	</div><br>

	<!-- Awards -->
	<div class="container">
		<h3 id="Awards" style="">Selected Awards</h3>
		<hr>
		<ul>
			<li><strong>EMNLP 2024 Diversity and Inclusion Award</strong>, 2023</li>
			<li><strong>NeurIPS 2023 Scholar Award</strong>, 2023</li>
			<li><strong>Bloomberg PhD Fellowship</strong>, 2023</li>
			<li><strong>Qualcomm Innovation Fellowship</strong>, 2023</li>
			<li><strong>UCLA Dissertation Year Fellowship</strong>, 2023</li>
			<li><strong>Amazon PhD Fellowship</strong>, 2023</li>
			<li><strong>NeurIPS 2022 Scholar Award</strong>, 2022</li>
			<!-- <li><strong>AAAI Travel Award</strong>, 2023</li> -->
			<li><strong>NeurIPS 2022 Top Reviewer</strong>, 2022</li>
			<!-- <li><strong>NeurIPS 2021 Scholar Award</strong>, 2021</li> -->
			<li><strong>ICLR 2022 Highlighted Reviewer</strong>, 2022</li>
			<li><strong>ACL-IJCNLP 2021 Diversity & Inclusion Award</strong>, 2021</li>
			<!-- <li><strong>NeurIPS Travel Award</strong>, 2019</li> -->
			<!-- <li><strong>ICCV Travel Award</strong>, 2019</li> -->
			<li><strong>Outstanding Master Thesis Award</strong>, Tsinghua University, 2018</li>
			<li><strong>Outstanding Graduate Award of Computer Science</strong>, Tsinghua University, 2018</li>
			<!-- <li><strong>SIGKDD Travel Award</strong>, 2018</li> -->
			<li><strong>"Stars of Tomorrow" Excellent Intern Award</strong>, Microsoft Research, 2018</li>
			<!-- <li><strong>AAAI Travel Award</strong>, 2018, 2020</li> -->
			<li><strong>GuangHua Scholarship Award</strong>, Tsinghua University, 2016</li>
			<li><strong>Outstanding Undergraduate, Beijing</strong>, 2015</li>
			<li><strong>Champion of National College Student Innovation Conference</strong>, 2014</li>
			<li><strong>National Scholarship</strong> (<em>for top 2% students</em>), 2014</li>
			<li><strong>First Prize of America Mathematical Contest In Modeling (MCM)</strong>, 2014</li>
			<li><strong>Xu Teli President Scholarship</strong> (<em>the highest honor for top 2
					undergraduates</em>), 2013</li>
		</ul>
	</div><br>

	<!-- Tweets -->
	<!-- https://publish.twitter.com/?buttonType=FollowButton&query=%40lupantech&widget=Timeline -->
	<!-- <div class="container">
		<h3 id="Tweets" style="">Tweets</h3>
		<hr>
		<ul>
			<div class="container" style="text-align: left;">
				<a class="twitter-timeline" data-width="50%" data-height="500" data-theme="light"
					href="https://twitter.com/lupantech?ref_src=twsrc%5Etfw">Tweets by lupantech</a>
				<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
			</div>
		</ul>
	</div><br> -->

	<!-- Contact -->
	<div class="container">
		<h3 id="Contact" style="">Contact</h3>
		<hr>
		UCLA Computer Science Department <br>
		404 Westwood Plaza <br>
		Los Angeles, CA 90095 <br>
		lupantech [at] gmail [dot] com <br>
		<!-- <a href="mailto:lupantech@gmail.com">lupantech@gmail.com</a><br> -->
		<a href="https://scholar.google.com/citations?user=IyucsdQAAAAJ&hl=en" target="_blank">[<span
				style="color:#4285F4">G</span><span style="color:#DB4437">o</span><span
				style="color:#F4B400">o</span><span style="color:#4285F4">g</span><span
				style="color:#0F9D58">l</span><span style="color:#DB4437">e</span> Scholar]
		</a> &nbsp;| &nbsp;
		<a href="https://www.semanticscholar.org/author/Pan-Lu/2887562" target="_blank">[Semantic Scholar]</a> &nbsp;|&nbsp;
		<a href="https://github.com/lupantech" target="_blank">[GitHub]</a> &nbsp;|&nbsp;
		<a href="https://www.linkedin.com/in/pan-lu-9308909a/" target="_blank">[LinkedIn]</a>
	</div>


	<!-- Footer -->
	<div class="container">
		<hr>
		<!-- <center>
			<footer>
				<p>&copy; Pan Lu 2019</p>
			</footer>
		</center> -->
		<div class="row">
			<div class="col-md-9">
				<p align="left">&copy; Pan Lu 2024</p>
			</div>
			<div class="col-md-3">
				<script type="text/javascript" id="clustrmaps"
					src="//cdn.clustrmaps.com/map_v2.js?d=kM7hb210psDEOVXSuHck6GmcfISVxTpZ0I6EmgBHZMQ&cl=ffffff&w=a"></script>
			</div>
		</div>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
		integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
		crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
		integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
		crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
		integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
		crossorigin="anonymous"></script>

	<script>
function showMore(){
  var dots = document.getElementById('dots');
  var moreContent = document.getElementById('more');
  var showMoreBtn = document.getElementById('showMoreBtn');

  if (dots.style.display === "none") {
    dots.style.display = "inline";
    showMoreBtn.innerHTML = "Show more";
    moreContent.style.display = "none";
  } else {
    dots.style.display = "none";
    showMoreBtn.innerHTML = "Show less";
    moreContent.style.display = "inline";
  }
}
	</script>

</body>

</html>